{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `Tokenizer` class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Get some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define url\n",
    "url = \"https://madridnyc.es/blogs-interesantes-e-influyentes-espanol/\"\n",
    "\n",
    "# Define headers for requests\n",
    "headers = {\n",
    "    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a request\n",
    "r = requests.get(url, headers = headers)\n",
    "\n",
    "# Create a beutiful soup\n",
    "soup = BeautifulSoup(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Transform the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all HTML <p>'s\n",
    "some_texts = soup.find_all('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the text - remove HTML tags\n",
    "training_text = []\n",
    "\n",
    "for text in some_texts:\n",
    "    training_text.append(text.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a tokenizer\n",
    "tokenizer = Tokenizer(num_words = 3000, oov_token = \"<OOV>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the tokenizer\n",
    "tokenizer.fit_on_texts(training_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1114"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many tokens do we have?\n",
    "tokens = tokenizer.word_index\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Let's see how it works on test data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some test data\n",
    "new_text = ['Estoy muy cansado, pero no se porque', \n",
    "            'No tenemos mucho tiempo', \n",
    "            'grandes o pequeños?', \n",
    "            'Quiero alcanzar algo increíble contigo!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform texts to sequences of numbers\n",
    "sequences = tokenizer.texts_to_sequences(training_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "371 64 81 2 35 82 3 102 2 58 6 59 31 83 372 1 204 32 373 2 1 37 134 60 24 3 38 205 61 374 84 375 6 376 377 1 135 101 1 378 379 35 27 136 "
     ]
    }
   ],
   "source": [
    "# Examine the transformed output - it should be a list of numbers (tokens)\n",
    "for i in sequences[1]:\n",
    "    print(i, end = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce a test sequence\n",
    "test_seq = tokenizer.texts_to_sequences(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1114, 32, 1114, 60, 59, 23, 1114],\n",
       " [59, 1114, 40, 202],\n",
       " [323, 61, 501],\n",
       " [1114, 912, 740, 1114, 1114]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the test seq\n",
    "test_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case 1\n",
      "Original sentence: Estoy muy cansado, pero no se porque\n",
      "Recreated sentence: <OOV> muy <OOV> pero no se <OOV> \n",
      "\n",
      "Case 2\n",
      "Original sentence: No tenemos mucho tiempo\n",
      "Recreated sentence: no <OOV> mucho tiempo \n",
      "\n",
      "Case 3\n",
      "Original sentence: grandes o pequeños?\n",
      "Recreated sentence: grandes o pequeños \n",
      "\n",
      "Case 4\n",
      "Original sentence: Quiero alcanzar algo increíble contigo!\n",
      "Recreated sentence: <OOV> alcanzar algo <OOV> <OOV> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Translate the test seq back to words\n",
    "for i, sentence in enumerate(test_seq):\n",
    "    print(f'Case {i + 1}')\n",
    "    print(f'Original sentence: {new_text[i]}')\n",
    "    print('Recreated sentence: ', end = '')\n",
    "    for word in sentence:\n",
    "        print(list(tokens.keys())[word - 1], end = ' ')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the texts come from an article about the best Spanish blogs, \n",
    "# which is written in not very personal tone, it's not surprising\n",
    "# that first-person verb forms are missing in our dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Using padding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1114,   32, 1114,   60,   59,   23, 1114],\n",
       "       [   0,    0,    0,   59, 1114,   40,  202],\n",
       "       [   0,    0,    0,    0,  323,   61,  501],\n",
       "       [   0,    0, 1114,  912,  740, 1114, 1114]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Padding adds zeros in front of the shorter sequences, so they become as long as the longest seq\n",
    "pad_sequences(test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_test = pad_sequences(test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case 1\n",
      "Original sentence: Estoy muy cansado, pero no se porque\n",
      "Recreated sentence: <OOV> muy <OOV> pero no se <OOV> \n",
      "\n",
      "Case 2\n",
      "Original sentence: No tenemos mucho tiempo\n",
      "Recreated sentence: <OOV> <OOV> <OOV> no <OOV> mucho tiempo \n",
      "\n",
      "Case 3\n",
      "Original sentence: grandes o pequeños?\n",
      "Recreated sentence: <OOV> <OOV> <OOV> <OOV> grandes o pequeños \n",
      "\n",
      "Case 4\n",
      "Original sentence: Quiero alcanzar algo increíble contigo!\n",
      "Recreated sentence: <OOV> <OOV> <OOV> alcanzar algo <OOV> <OOV> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Padding does not affect back-translation to words\n",
    "for i, sentence in enumerate(padded_test):\n",
    "    print(f'Case {i + 1}')\n",
    "    print(f'Original sentence: {new_text[i]}')\n",
    "    print('Recreated sentence: ', end = '')\n",
    "    for word in sentence:\n",
    "        print(list(tokens.keys())[word - 1], end = ' ')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Other parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `padding = post` & other options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1114,   32, 1114,   60,   59,   23, 1114],\n",
       "       [  59, 1114,   40,  202,    0,    0,    0],\n",
       "       [ 323,   61,  501,    0,    0,    0,    0],\n",
       "       [1114,  912,  740, 1114, 1114,    0,    0]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can also add zeros at the end of your sentences\n",
    "pad_sequences(test_seq, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1114,   32, 1114,   60,   59,   23, 1114,    0,    0,    0],\n",
       "       [  59, 1114,   40,  202,    0,    0,    0,    0,    0,    0],\n",
       "       [ 323,   61,  501,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [1114,  912,  740, 1114, 1114,    0,    0,    0,    0,    0]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can also specify maximum length of a sequence\n",
    "pad_sequences(test_seq, padding = 'post', maxlen = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1114,   32, 1114],\n",
       "       [  59, 1114,   40],\n",
       "       [ 323,   61,  501],\n",
       "       [1114,  912,  740]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ...and specify where the sentence should be truncated if it's longer than `maxlen`\n",
    "pad_sequences(test_seq, padding = 'post', maxlen = 3, truncating = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
